

          X1 ------------------- 游댯
                                   --------------- 游댮   
          X2 ------------------- 游댯                 ---------------- 游릭
( Input Variables )                --------------- 游댮                 -------------- 游리 => Y ( output variable )
          X3 ------------------- 游댯                 ---------------- 游릭
                                   --------------- 游댮
          X4 ------------------- 游댯

  游댯( Input Layer )                                 游댮游릭 ( Hidden layers )              游리( output Layer )  

  And all the three layers are assumed as Brain

  => It perform all possible patterns and consider the weight according to Accuracy

      Input Layers  -------> Hidden layer -------> output Layer


  => In NN weight is called Kernel

  * the learning in which we learn with guidence or by knowing the value of Y is said to be "supervised Learning".

  => whenever we dont get the actual value of y then the process get reversed( initial ) to change the weights. This 
      process of coming back is called "Back propogation" and changing the weight is called "Weight Reinitialize" .

  => Reprocessing the hidden layers to relearn from the new weights is called "Feed Farward" .

  => Optimizers is the one that controls initializers to get required weight quickly. is responsible for updating the weights and biases of the model during training to reduce the error between the predicted output and the actual target output. 
        The goal is to find the optimal set of weights that minimizes the loss function and improves the model's performance on the training data.

  => Activation function decides to consider the input of neuron and activate accordingly.
      they play a crucial role in enabling neural networks to learn and approximate complex non-linear relationships in the data. 
      Without activation functions, the neural network would behave like a linear model, making it limited in its ability to learn from and represent complex patterns in the data.
                   
      Error  = actual value - outcome value

Accuracy increases with increase in number of layers

=> sklearn is built on the library Numpy
=> Keras is built on the library Tensorflow



      
