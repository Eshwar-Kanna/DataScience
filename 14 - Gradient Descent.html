

=> As the layers increase the Accuracy Increases But as a layer the accuracy doesnot make much difference. By adding a layer 
    their we stop increasing layers

=> Aim is to find the weight where their is minimum loss. Learning rate of model should be according to weight at minimum loss

=> Gradient Descent is the optimization algorithm for finding the weight that gives minimum loss and it process continuously untill 
    the weight obtained

=> SGD : Stochastic gradient Descent is an iterative method of optimizing loss. It optimizes one by one thus it takes long time and we get global minima.
=> Optimizing loss by dumping entire dataset all at once takes shorter time but cannot obtain global minima
=> To find global minima in short time we create batches with combining group mentioning batch size
